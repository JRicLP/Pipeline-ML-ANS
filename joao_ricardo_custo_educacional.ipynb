{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSdgFAt4cBR3Sg0FjTsOq7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JRicLP/Pipeline-ML-ANS/blob/main/joao_ricardo_custo_educacional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "WzGlr90CtY9Z"
      },
      "outputs": [],
      "source": [
        "#Bibliotecas usadas:\n",
        "try:\n",
        "  import gdown\n",
        "except ImportError:\n",
        "  !pip install gdown\n",
        "  import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, silhouette_score\n",
        "from sklearn.utils import resample\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "file_id = '1GBru39-ASKE52R8gpEZrVwlROPRwXtAw'\n",
        "dataset_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Definir nome do arquivo:\n",
        "output_path = 'credit_data.csv'\n",
        "\n",
        "# Baixar o arquivo usando gdown:\n",
        "print(f\"Baixando o dataset do Google Drive...\")\n",
        "gdown.download(dataset_url, output_path, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "uewWXAIJt7wf",
        "outputId": "8d47cd3a-084d-46b4-c1e3-5fff9a27f5f5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando o dataset do Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GBru39-ASKE52R8gpEZrVwlROPRwXtAw\n",
            "To: /content/credit_data.csv\n",
            "100%|██████████| 84.1k/84.1k [00:00<00:00, 3.90MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'credit_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o arquivo:\n",
        "try:\n",
        "    with open(output_path, 'r') as file:\n",
        "        first_line = file.readline().strip()\n",
        "\n",
        "    if ',' in first_line and ';' not in first_line:\n",
        "        dataset = pd.read_csv(output_path, sep=None, engine='python')\n",
        "    else:\n",
        "        dataset = pd.read_csv(output_path, sep=';')\n",
        "\n",
        "        if dataset.shape[1] == 1:\n",
        "            first_col_name = dataset.columns[0]\n",
        "            temp_df = dataset[first_col_name].str.split(',', expand=True)\n",
        "\n",
        "            if 'Country' in first_col_name or 'ID' in first_col_name:\n",
        "                header = first_col_name.split(',')\n",
        "                if len(header) < temp_df.shape[1]:\n",
        "                    for i in range(len(header), temp_df.shape[1]):\n",
        "                        header.append(f'Column_{i}')\n",
        "                temp_df.columns = header\n",
        "\n",
        "            dataset = temp_df\n",
        "\n",
        "    print(\"Dataset carregado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar o dataset: {e}\")\n",
        "    print(\"Tentando outro método de carregamento...\")\n",
        "    try:\n",
        "        dataset = pd.read_csv(output_path, sep=None, engine='python')\n",
        "        print(\"Dataset carregado com sucesso usando detecção automática de separador!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Erro final ao carregar o dataset: {e2}\")\n",
        "\n",
        "# Verificando se as colunas estão corretamente separadas:\n",
        "print(\"\\nVerificando estrutura do dataset...\")\n",
        "print(f\"Número de colunas: {dataset.shape[1]}\")\n",
        "print(f\"Nomes das colunas: {dataset.columns.tolist()}\")\n",
        "\n",
        "for col in dataset.columns:\n",
        "    if col not in ['Country', 'City', 'University', 'Program', 'Level'] and 'ID' not in col:\n",
        "        try:\n",
        "            dataset[col] = pd.to_numeric(dataset[col], errors='coerce')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Verificando se há colunas numéricas para prosseguir com a análise:\n",
        "numeric_cols = dataset.select_dtypes(include=['number']).columns\n",
        "print(f\"\\nColunas numéricas disponíveis: {numeric_cols.tolist()}\")\n",
        "\n",
        "if len(numeric_cols) == 0:\n",
        "    print(\"ERRO: Não foram encontradas colunas numéricas no dataset.\")\n",
        "    print(\"Verifique o formato do arquivo CSV e o separador usado.\")\n",
        "    print(\"Criando colunas numéricas artificiais para teste...\")\n",
        "    dataset['Valor_1'] = np.random.uniform(0, 100, size=len(dataset))\n",
        "    dataset['Valor_2'] = np.random.uniform(0, 200, size=len(dataset))\n",
        "    dataset['Valor_3'] = np.random.uniform(0, 50, size=len(dataset))\n",
        "    numeric_cols = ['Valor_1', 'Valor_2', 'Valor_3']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwkKHR3xuBlf",
        "outputId": "cab20de6-b6d4-441a-b986-b5979500b460"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset carregado com sucesso!\n",
            "\n",
            "Verificando estrutura do dataset...\n",
            "Número de colunas: 12\n",
            "Nomes das colunas: ['Country', 'City', 'University', 'Program', 'Level', 'Duration_Years', 'Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n",
            "\n",
            "Colunas numéricas disponíveis: ['Duration_Years', 'Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando e convertendo tipos de dados após carregar o dataset:\n",
        "numeric_columns = ['Duration_Years', 'Tuition_USD', 'Living_Cost_Index',\n",
        "                  'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n",
        "\n",
        "for col in numeric_columns:\n",
        "    if col in dataset.columns:\n",
        "        dataset[col] = pd.to_numeric(dataset[col], errors='coerce')\n",
        "\n",
        "# Verificando os tipos após a conversão:\n",
        "print(\"\\nTipos de dados após conversão:\")\n",
        "print(dataset.dtypes)\n",
        "\n",
        "# Exibindo o DataFrame:\n",
        "print(\"\\nPrimeiras Linhas do Dataset:\")\n",
        "print(dataset.head().to_string())\n",
        "\n",
        "# Obtendo informações gerais do dataset:\n",
        "print(\"\\nInformações Gerais do Dataset:\")\n",
        "dataset.info()\n",
        "\n",
        "# Exibindo o número de linhas e colunas:\n",
        "print(f\"\\nDimensões do Dataset: {dataset.shape} (Linhas, Colunas)\")\n",
        "\n",
        "# Exibindo o nome das colunas:\n",
        "print(\"\\nNomes das Colunas:\")\n",
        "print(dataset.columns.tolist())\n",
        "\n",
        "# Checando os dados ausentes por coluna, caso tenha:\n",
        "print(\"\\nDados Ausentes por Coluna:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Exibindo tipos de dados de cada coluna:\n",
        "print(\"\\nTipos de Dados das Colunas:\")\n",
        "print(dataset.dtypes)\n",
        "\n",
        "# Exibindo estatísticas descritivas para colunas numéricas:\n",
        "print(\"\\nEstatísticas Descritivas das Colunas Numéricas:\")\n",
        "print(dataset.describe().to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbZzcH8uHuW",
        "outputId": "25c8d9cc-4426-4fa9-d805-5259445db9e6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tipos de dados após conversão:\n",
            "Country               object\n",
            "City                  object\n",
            "University            object\n",
            "Program               object\n",
            "Level                 object\n",
            "Duration_Years       float64\n",
            "Tuition_USD            int64\n",
            "Living_Cost_Index    float64\n",
            "Rent_USD               int64\n",
            "Visa_Fee_USD           int64\n",
            "Insurance_USD          int64\n",
            "Exchange_Rate        float64\n",
            "dtype: object\n",
            "\n",
            "Primeiras Linhas do Dataset:\n",
            "     Country       City                      University                 Program   Level  Duration_Years  Tuition_USD  Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate\n",
            "0        USA  Cambridge              Harvard University        Computer Science  Master             2.0        55400               83.5      2200           160           1500           1.00\n",
            "1         UK     London         Imperial College London            Data Science  Master             1.0        41200               75.8      1800           485            800           0.79\n",
            "2     Canada    Toronto           University of Toronto      Business Analytics  Master             2.0        38500               72.5      1600           235            900           1.35\n",
            "3  Australia  Melbourne         University of Melbourne             Engineering  Master             2.0        42000               71.2      1400           450            650           1.52\n",
            "4    Germany     Munich  Technical University of Munich  Mechanical Engineering  Master             2.0          500               70.5      1100            75            550           0.92\n",
            "\n",
            "Informações Gerais do Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 907 entries, 0 to 906\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   Country            907 non-null    object \n",
            " 1   City               907 non-null    object \n",
            " 2   University         907 non-null    object \n",
            " 3   Program            907 non-null    object \n",
            " 4   Level              907 non-null    object \n",
            " 5   Duration_Years     907 non-null    float64\n",
            " 6   Tuition_USD        907 non-null    int64  \n",
            " 7   Living_Cost_Index  907 non-null    float64\n",
            " 8   Rent_USD           907 non-null    int64  \n",
            " 9   Visa_Fee_USD       907 non-null    int64  \n",
            " 10  Insurance_USD      907 non-null    int64  \n",
            " 11  Exchange_Rate      907 non-null    float64\n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 85.2+ KB\n",
            "\n",
            "Dimensões do Dataset: (907, 12) (Linhas, Colunas)\n",
            "\n",
            "Nomes das Colunas:\n",
            "['Country', 'City', 'University', 'Program', 'Level', 'Duration_Years', 'Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n",
            "\n",
            "Dados Ausentes por Coluna:\n",
            "Country              0\n",
            "City                 0\n",
            "University           0\n",
            "Program              0\n",
            "Level                0\n",
            "Duration_Years       0\n",
            "Tuition_USD          0\n",
            "Living_Cost_Index    0\n",
            "Rent_USD             0\n",
            "Visa_Fee_USD         0\n",
            "Insurance_USD        0\n",
            "Exchange_Rate        0\n",
            "dtype: int64\n",
            "\n",
            "Tipos de Dados das Colunas:\n",
            "Country               object\n",
            "City                  object\n",
            "University            object\n",
            "Program               object\n",
            "Level                 object\n",
            "Duration_Years       float64\n",
            "Tuition_USD            int64\n",
            "Living_Cost_Index    float64\n",
            "Rent_USD               int64\n",
            "Visa_Fee_USD           int64\n",
            "Insurance_USD          int64\n",
            "Exchange_Rate        float64\n",
            "dtype: object\n",
            "\n",
            "Estatísticas Descritivas das Colunas Numéricas:\n",
            "       Duration_Years   Tuition_USD  Living_Cost_Index     Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate\n",
            "count      907.000000    907.000000         907.000000   907.000000    907.000000     907.000000     907.000000\n",
            "mean         2.836825  16705.016538          64.437486   969.206174    211.396913     700.077178     623.000695\n",
            "std          0.945449  16582.385275          14.056333   517.154752    143.435740     320.374875    3801.746134\n",
            "min          1.000000      0.000000          27.800000   150.000000     40.000000     200.000000       0.150000\n",
            "25%          2.000000   2850.000000          56.300000   545.000000    100.000000     450.000000       0.920000\n",
            "50%          3.000000   7500.000000          67.500000   900.000000    160.000000     650.000000       1.350000\n",
            "75%          4.000000  31100.000000          72.200000  1300.000000    240.000000     800.000000       7.150000\n",
            "max          5.000000  62000.000000         122.400000  2500.000000    490.000000    1500.000000   42150.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Pré-processamento dos dados:\n",
        "\n",
        "# Identificando colunas numéricas:\n",
        "numeric_columns = dataset.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Tratamento de valores nulos (código original mantido por robustez)\n",
        "for col in dataset.columns:\n",
        "    if dataset[col].isnull().sum() > 0:\n",
        "        if col in numeric_columns:\n",
        "            if 'ID' in col:\n",
        "                dataset[col] = dataset[col].fillna(dataset[col].max() + 1)\n",
        "            elif 'Idade' in col:\n",
        "                dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
        "            elif 'Renda' in col or 'Valor' in col:\n",
        "                dataset[col] = dataset[col].fillna(dataset[col].mean())\n",
        "            elif 'Pontuacao' in col:\n",
        "                dataset[col] = dataset[col].fillna(dataset[col].max())\n",
        "            else:\n",
        "                dataset[col] = dataset[col].fillna(dataset[col].mean())\n",
        "        else:\n",
        "            dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
        "\n",
        "print(\"Valores nulos após tratamento:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Detecção de outliers (código original)\n",
        "print(\"\\nDetectando outliers...\")\n",
        "limite = 1.5\n",
        "numeric_cols = dataset.select_dtypes(include='number')\n",
        "outlier_counts = 0\n",
        "total_values = numeric_cols.size\n",
        "\n",
        "for col in numeric_cols.columns:\n",
        "    if 'ID' not in col:\n",
        "        Q1 = numeric_cols[col].quantile(0.25)\n",
        "        Q3 = numeric_cols[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - limite * IQR\n",
        "        upper_bound = Q3 + limite * IQR\n",
        "        col_outliers = numeric_cols[(numeric_cols[col] < lower_bound) | (numeric_cols[col] > upper_bound)][col].count()\n",
        "        outlier_counts += col_outliers\n",
        "        print(f\"Coluna '{col}' - Outliers: {col_outliers} ({(col_outliers / len(dataset) * 100):.2f}%)\")\n",
        "\n",
        "if total_values > 0:\n",
        "    outlier_percentage = (outlier_counts / total_values) * 100\n",
        "    print(f\"\\nPercentual de contaminação de outliers no dataset: {outlier_percentage:.2f}%\")\n",
        "else:\n",
        "    print(\"\\nNenhuma coluna numérica encontrada para detecção de outliers.\")\n",
        "\n",
        "# Visualização de outliers (código original)\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols.columns):\n",
        "    if 'ID' not in col and i < 9:\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.boxplot(dataset[col])\n",
        "        plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.savefig('boxplots.png')\n",
        "plt.close()\n",
        "print(\"Boxplots das features numéricas salvos em 'boxplots.png'\")\n",
        "\n",
        "# Tratamento dos outliers (winsorização) (código original)\n",
        "print(\"\\nTratando outliers através de winsorização...\")\n",
        "for col in numeric_cols.columns:\n",
        "    if 'ID' not in col:\n",
        "        Q1 = numeric_cols[col].quantile(0.25)\n",
        "        Q3 = numeric_cols[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - limite * IQR\n",
        "        upper_bound = Q3 + limite * IQR\n",
        "        dataset[col] = np.where(dataset[col] < lower_bound, lower_bound, dataset[col])\n",
        "        dataset[col] = np.where(dataset[col] > upper_bound, upper_bound, dataset[col])\n",
        "print(\"Outliers tratados com sucesso.\")\n",
        "\n",
        "# 2.5. Codificação de Features Categóricas\n",
        "# Esta etapa é essencial para que os algoritmos de ML possam processar as colunas de texto.\n",
        "\n",
        "print(\"\\n2.5. Codificando features categóricas (texto)...\")\n",
        "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Vamos guardar os \"LabelEncoders\" para usar depois em novos dados\n",
        "encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    dataset[col] = le.fit_transform(dataset[col])\n",
        "    encoders[col] = le # Salva o encoder treinado\n",
        "    print(f\"Coluna '{col}' codificada com LabelEncoder.\")\n",
        "\n",
        "print(\"\\nDataset após codificação categórica:\")\n",
        "print(dataset.head().to_string())\n",
        "\n",
        "print(\"\\nTipos de dados finais (prontos para ML):\")\n",
        "print(dataset.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7WQrUmMuOvg",
        "outputId": "35815282-ff6e-44a3-fe11-4e5ae1391c99"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores nulos após tratamento:\n",
            "Country              0\n",
            "City                 0\n",
            "University           0\n",
            "Program              0\n",
            "Level                0\n",
            "Duration_Years       0\n",
            "Tuition_USD          0\n",
            "Living_Cost_Index    0\n",
            "Rent_USD             0\n",
            "Visa_Fee_USD         0\n",
            "Insurance_USD        0\n",
            "Exchange_Rate        0\n",
            "dtype: int64\n",
            "\n",
            "Detectando outliers...\n",
            "Coluna 'Duration_Years' - Outliers: 0 (0.00%)\n",
            "Coluna 'Tuition_USD' - Outliers: 0 (0.00%)\n",
            "Coluna 'Living_Cost_Index' - Outliers: 20 (2.21%)\n",
            "Coluna 'Rent_USD' - Outliers: 3 (0.33%)\n",
            "Coluna 'Visa_Fee_USD' - Outliers: 93 (10.25%)\n",
            "Coluna 'Insurance_USD' - Outliers: 78 (8.60%)\n",
            "Coluna 'Exchange_Rate' - Outliers: 169 (18.63%)\n",
            "\n",
            "Percentual de contaminação de outliers no dataset: 5.72%\n",
            "Boxplots das features numéricas salvos em 'boxplots.png'\n",
            "\n",
            "Tratando outliers através de winsorização...\n",
            "Outliers tratados com sucesso.\n",
            "\n",
            "2.5. Codificando features categóricas (texto)...\n",
            "Coluna 'Country' codificada com LabelEncoder.\n",
            "Coluna 'City' codificada com LabelEncoder.\n",
            "Coluna 'University' codificada com LabelEncoder.\n",
            "Coluna 'Program' codificada com LabelEncoder.\n",
            "Coluna 'Level' codificada com LabelEncoder.\n",
            "\n",
            "Dataset após codificação categórica:\n",
            "   Country  City  University  Program  Level  Duration_Years  Tuition_USD  Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate\n",
            "0       66    84         101       17      1             2.0      55400.0               83.5    2200.0         160.0         1325.0           1.00\n",
            "1       65   283         128       27      1             1.0      41200.0               75.8    1800.0         450.0          800.0           0.79\n",
            "2        9   503         572        9      1             2.0      38500.0               72.5    1600.0         235.0          900.0           1.35\n",
            "3        2   313         495       44      1             2.0      42000.0               71.2    1400.0         450.0          650.0           1.52\n",
            "4       22   330         307       66      1             2.0        500.0               70.5    1100.0          75.0          550.0           0.92\n",
            "\n",
            "Tipos de dados finais (prontos para ML):\n",
            "Country                int64\n",
            "City                   int64\n",
            "University             int64\n",
            "Program                int64\n",
            "Level                  int64\n",
            "Duration_Years       float64\n",
            "Tuition_USD          float64\n",
            "Living_Cost_Index    float64\n",
            "Rent_USD             float64\n",
            "Visa_Fee_USD         float64\n",
            "Insurance_USD        float64\n",
            "Exchange_Rate        float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Normalização dos dados\n",
        "\n",
        "# Normalizando colunas:\n",
        "columns_to_normalize = [col for col in numeric_cols if 'ID' not in col]\n",
        "if 'Pontuacao_Credito' in dataset.columns:\n",
        "    potential_target = 'Pontuacao_Credito'\n",
        "    if potential_target in columns_to_normalize:\n",
        "        columns_to_normalize.remove(potential_target)\n",
        "\n",
        "# Aplicando normalização Min-Max:\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalizando apenas as colunas selecionadas:\n",
        "dataset[columns_to_normalize] = scaler.fit_transform(dataset[columns_to_normalize])\n",
        "\n",
        "print(f\"Normalização aplicada às seguintes colunas: {columns_to_normalize}\")\n",
        "print(\"\\nPrimeiras linhas após normalização:\")\n",
        "print(dataset.head().to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkNWdIhcuY3u",
        "outputId": "173891d0-d8a0-461b-d1a2-b83f6a565920"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalização aplicada às seguintes colunas: ['Duration_Years', 'Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n",
            "\n",
            "Primeiras linhas após normalização:\n",
            "   Country  City  University  Program  Level  Duration_Years  Tuition_USD  Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate\n",
            "0       66    84         101       17      1            0.25     0.893548           0.802673  0.898138      0.292683       1.000000       0.052004\n",
            "1       65   283         128       27      1            0.00     0.664516           0.681604  0.722892      1.000000       0.533333       0.039156\n",
            "2        9   503         572        9      1            0.25     0.620968           0.629717  0.635268      0.475610       0.622222       0.073417\n",
            "3        2   313         495       44      1            0.25     0.677419           0.609277  0.547645      1.000000       0.400000       0.083818\n",
            "4       22   330         307       66      1            0.25     0.008065           0.598270  0.416210      0.085366       0.311111       0.047109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Aprendizado Semi-Supervisionado com KMeans:\n",
        "\n",
        "# 4.1. Definindo features para clustering:\n",
        "print(\"\\n4.1. Preparando dados para clustering...\")\n",
        "\n",
        "# Definir explicitamente as colunas para o cluster (perfil de custo)\n",
        "cluster_columns = ['Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD']\n",
        "print(f\"Colunas usadas para clustering: {cluster_columns}\")\n",
        "\n",
        "X_cluster_raw = dataset[cluster_columns].copy()\n",
        "\n",
        "# Normalização (ESSENCIAL para o KMeans)\n",
        "print(\"Normalizando dados para o KMeans...\")\n",
        "scaler_cluster = MinMaxScaler()\n",
        "X_cluster_normalized = scaler_cluster.fit_transform(X_cluster_raw)\n",
        "# Salvar o scaler\n",
        "with open('kmeans_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_cluster, f)\n",
        "\n",
        "# Visualizando os dados (PCA com dados normalizados)\n",
        "print(\"Gerando visualização PCA...\")\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_cluster_normalized)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
        "plt.title('Visualização dos dados (Custos Normalizados) com PCA')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.savefig('pca_before_clustering.png')\n",
        "plt.close()\n",
        "print(\"Visualização PCA salva em 'pca_before_clustering.png'\")\n",
        "\n",
        "\n",
        "# 4.2. Determinando o número ideal de clusters:\n",
        "print(\"Determinando número ideal de clusters (K)...\")\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "max_clusters = min(10, len(dataset) - 1)\n",
        "\n",
        "for k in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_cluster_normalized)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "    if k > 1:\n",
        "        silhouette_avg = silhouette_score(X_cluster_normalized, kmeans.labels_)\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "        print(f\"Para {k} clusters, o silhouette score é: {silhouette_avg:.4f}\")\n",
        "\n",
        "# (Código de plotagem do cotovelo e silhueta mantido)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(2, max_clusters + 1), inertia, 'bo-')\n",
        "plt.title('Método do Cotovelo')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(2, max_clusters + 1), silhouette_scores, 'ro-')\n",
        "plt.title('Silhouette Score')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.tight_layout()\n",
        "plt.savefig('elbow_method.png')\n",
        "plt.close()\n",
        "print(\"Gráfico do método do cotovelo salvo em 'elbow_method.png'\")\n",
        "\n",
        "# --- INÍCIO DA CORREÇÃO (k=4) ---\n",
        "# Escolhendo o número de clusters com base no maior silhouette score:\n",
        "optimal_k_sugestao = silhouette_scores.index(max(silhouette_scores)) + 2\n",
        "print(f\"\\nNúmero ideal de clusters (sugestão do Silhouette): {optimal_k_sugestao}\")\n",
        "\n",
        "# Substituir pelo valor fixo para melhor interpretação de negócio\n",
        "optimal_k = 4\n",
        "print(f\"SUBSTITUINDO: Usando k={optimal_k} clusters fixos para melhor interpretação.\")\n",
        "# --- FIM DA CORREÇÃO ---\n",
        "\n",
        "\n",
        "# 4.3. Realizando o clustering com o número ideal de clusters:\n",
        "print(f\"\\n4.3. Realizando clustering com {optimal_k} clusters...\")\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_cluster_normalized)\n",
        "\n",
        "dataset['Cluster'] = clusters\n",
        "print(\"Clusters atribuídos ao dataset.\")\n",
        "\n",
        "# (Código de plotagem dos clusters mantido)\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.8)\n",
        "plt.title(f'Visualização dos {optimal_k} Clusters com PCA')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.savefig('clusters_pca.png')\n",
        "plt.close()\n",
        "print(\"Visualização dos clusters salva em 'clusters_pca.png'\")\n",
        "\n",
        "\n",
        "# 4.4. Analisando características de cada cluster:\n",
        "print(\"\\n4.4. Analisando características de cada cluster...\")\n",
        "# Usar valores originais (não-normalizados) para entender o significado\n",
        "cluster_analysis = dataset.groupby('Cluster')[cluster_columns].mean()\n",
        "print(\"\\nMédia das features de CUSTO por cluster (valores originais):\")\n",
        "print(cluster_analysis.to_string())\n",
        "\n",
        "\n",
        "# 4.5. Rotulando os clusters semanticamente (baseados em Custo)\n",
        "print(\"\\n4.5. Atribuindo rótulos semânticos (baseados em Custo)...\")\n",
        "\n",
        "# Calcular uma pontuação de \"custo\"\n",
        "cluster_analysis['Custo_Total_Medio'] = cluster_analysis['Tuition_USD'] + cluster_analysis['Rent_USD']\n",
        "\n",
        "# Ordenar os clusters pelo custo total, do menor para o maior\n",
        "sorted_clusters_by_cost = cluster_analysis.sort_values(by='Custo_Total_Medio').index\n",
        "\n",
        "# Definir rótulos significativos (esta lógica agora será ativada)\n",
        "if optimal_k == 4:\n",
        "    cost_labels = ['Custo Baixo', 'Custo Medio-Baixo', 'Custo Medio-Alto', 'Custo Alto']\n",
        "elif optimal_k == 3:\n",
        "    cost_labels = ['Custo Baixo', 'Custo Medio', 'Custo Alto']\n",
        "else: # Fallback genérico (não deve ser usado agora)\n",
        "    cost_labels = [f\"Nivel_Custo_{i}\" for i in range(optimal_k)]\n",
        "\n",
        "# Mapear os IDs dos clusters ordenados para os rótulos de custo\n",
        "semantic_labels = {}\n",
        "for i, cluster_id in enumerate(sorted_clusters_by_cost):\n",
        "    semantic_labels[cluster_id] = cost_labels[i]\n",
        "\n",
        "print(\"\\nAtribuição de rótulos de Custo aos clusters:\")\n",
        "for cluster_id, label in semantic_labels.items():\n",
        "    print(f\"Cluster {cluster_id}: {label}\")\n",
        "\n",
        "# Adicionando os rótulos semânticos ao dataset:\n",
        "dataset['Rotulo_Custo'] = dataset['Cluster'].map(semantic_labels)\n",
        "\n",
        "# Exibindo a distribuição dos rótulos semânticos:\n",
        "print(\"\\nDistribuição dos rótulos de custo:\")\n",
        "print(dataset['Rotulo_Custo'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaY2xbDAucIO",
        "outputId": "edce1e34-07fd-41f7-b972-b0a486d1aa75"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4.1. Preparando dados para clustering...\n",
            "Colunas usadas para clustering: ['Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD']\n",
            "Normalizando dados para o KMeans...\n",
            "Gerando visualização PCA...\n",
            "Visualização PCA salva em 'pca_before_clustering.png'\n",
            "Determinando número ideal de clusters (K)...\n",
            "Para 2 clusters, o silhouette score é: 0.4333\n",
            "Para 3 clusters, o silhouette score é: 0.4843\n",
            "Para 4 clusters, o silhouette score é: 0.4762\n",
            "Para 5 clusters, o silhouette score é: 0.4868\n",
            "Para 6 clusters, o silhouette score é: 0.4921\n",
            "Para 7 clusters, o silhouette score é: 0.4493\n",
            "Para 8 clusters, o silhouette score é: 0.5010\n",
            "Para 9 clusters, o silhouette score é: 0.4309\n",
            "Para 10 clusters, o silhouette score é: 0.4060\n",
            "Gráfico do método do cotovelo salvo em 'elbow_method.png'\n",
            "\n",
            "Número ideal de clusters (sugestão do Silhouette): 8\n",
            "SUBSTITUINDO: Usando k=4 clusters fixos para melhor interpretação.\n",
            "\n",
            "4.3. Realizando clustering com 4 clusters...\n",
            "Clusters atribuídos ao dataset.\n",
            "Visualização dos clusters salva em 'clusters_pca.png'\n",
            "\n",
            "4.4. Analisando características de cada cluster...\n",
            "\n",
            "Média das features de CUSTO por cluster (valores originais):\n",
            "         Tuition_USD  Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD\n",
            "Cluster                                                                       \n",
            "0           0.144611           0.596707  0.367404      0.290102       0.492569\n",
            "1           0.736340           0.714887  0.716845      0.279492       0.902041\n",
            "2           0.500691           0.582255  0.488917      0.956794       0.463069\n",
            "3           0.068243           0.222825  0.103505      0.167159       0.142361\n",
            "\n",
            "4.5. Atribuindo rótulos semânticos (baseados em Custo)...\n",
            "\n",
            "Atribuição de rótulos de Custo aos clusters:\n",
            "Cluster 3: Custo Baixo\n",
            "Cluster 0: Custo Medio-Baixo\n",
            "Cluster 2: Custo Medio-Alto\n",
            "Cluster 1: Custo Alto\n",
            "\n",
            "Distribuição dos rótulos de custo:\n",
            "Rotulo_Custo\n",
            "Custo Medio-Baixo    343\n",
            "Custo Baixo          256\n",
            "Custo Medio-Alto     210\n",
            "Custo Alto            98\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Classificação utilizando os rótulos atribuídos (CORRIGIDO)\n",
        "\n",
        "# --- Definir Target e Features (sem Data Leakage) ---\n",
        "target_column = 'Rotulo_Custo'\n",
        "target = dataset[target_column]\n",
        "\n",
        "cost_columns = ['Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n",
        "utility_columns = ['Cluster', target_column]\n",
        "features = dataset.drop(columns = cost_columns + utility_columns)\n",
        "\n",
        "print(f\"\\nTarget (y) para classificação: '{target_column}'\")\n",
        "print(f\"Features (X) para classificação: {features.columns.tolist()}\")\n",
        "\n",
        "# --- CORREÇÃO: Codificar o Target (y) de String para Números ---\n",
        "# O MLPClassifier requer que os rótulos (y) sejam numéricos\n",
        "le_target = LabelEncoder()\n",
        "target_encoded = le_target.fit_transform(target) # <-- CORREÇÃO\n",
        "print(f\"\\nRótulos do Target codificados (ex: '{target[0]}' -> {target_encoded[0]})\")\n",
        "# Salvar o encoder do target para usar nas Células 6 e 7\n",
        "with open('target_label_encoder.pkl', 'wb') as f: # <-- CORREÇÃO\n",
        "    pickle.dump(le_target, f)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Verificando se há classes desbalanceadas:\n",
        "# Usamos o 'target' original (strings) para contagem, pois é mais legível\n",
        "print(f\"\\nDistribuição das classes (Target: {target_column}):\")\n",
        "class_distribution = target.value_counts()\n",
        "print(class_distribution)\n",
        "\n",
        "# Balanceando o dataset (Downsampling)\n",
        "if class_distribution.min() / class_distribution.max() < 0.5:\n",
        "    print(\"\\nDetectado desbalanceamento. Aplicando downsampling...\")\n",
        "\n",
        "    # Precisamos adicionar o target codificado ao features para o resample\n",
        "    features_with_target = features.copy() # <-- CORREÇÃO\n",
        "    features_with_target[target_column] = target_encoded # <-- CORREÇÃO\n",
        "\n",
        "    minority_class_label = class_distribution.idxmin()\n",
        "    # Mapear o rótulo da string (minoria) para seu valor codificado\n",
        "    minority_class_encoded = le_target.transform([minority_class_label])[0] # <-- CORREÇÃO\n",
        "    n_samples_minority = class_distribution.min()\n",
        "    balanced_df_list = []\n",
        "\n",
        "    # Usamos o target_encoded (0, 1, 2...) para o loop\n",
        "    for class_label_encoded in np.unique(target_encoded): # <-- CORREÇÃO\n",
        "        class_data = features_with_target[features_with_target[target_column] == class_label_encoded] # <-- CORREÇÃO\n",
        "\n",
        "        if class_label_encoded == minority_class_encoded: # <-- CORREÇÃO\n",
        "            balanced_df_list.append(class_data)\n",
        "        else:\n",
        "            downsampled_class_data = resample(class_data,\n",
        "                                             replace=False,\n",
        "                                             n_samples=n_samples_minority,\n",
        "                                             random_state=42)\n",
        "            balanced_df_list.append(downsampled_class_data)\n",
        "\n",
        "    balanced_dataset = pd.concat(balanced_df_list).reset_index(drop=True)\n",
        "\n",
        "    # Atualizando target e features com base no dataset balanceado\n",
        "    target_final = balanced_dataset[target_column] # <-- CORREÇÃO (Este é o y numérico)\n",
        "    features_final = balanced_dataset.drop(columns=[target_column]) # <-- CORREÇÃO (Este é o X)\n",
        "\n",
        "    print(\"\\nDistribuição das classes após balanceamento (rótulos codificados):\")\n",
        "    print(target_final.value_counts())\n",
        "else:\n",
        "    print(\"\\nDataset está suficientemente balanceado.\")\n",
        "    target_final = target_encoded # <-- CORREÇÃO\n",
        "    features_final = features # <-- CORREÇÃO\n",
        "\n",
        "\n",
        "# 5.2. Dividindo em conjuntos de Treino e Teste:\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_final, target_final, test_size=0.2, random_state=42, stratify=target_final # <-- CORREÇÃO\n",
        ")\n",
        "print(f\"\\nTamanho do conjunto de treino: {X_train.shape[0]} exemplos\")\n",
        "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]} exemplos\")\n",
        "print(f\"Tipo de y_train: {y_train.dtype}\") # <-- CORREÇÃO (Deve ser int)\n",
        "\n",
        "\n",
        "# 5.3 Support Vector Machine (SVM)\n",
        "print(\"\\n5.3.1. Treinando modelo SVM...\")\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='auto', random_state=42)\n",
        "svm_model.fit(X_train, y_train) # y_train agora é numérico\n",
        "print(\"Modelo SVM treinado com sucesso!\")\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Métricas\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_svm = precision_score(y_test, y_pred_svm, average='weighted', zero_division=0) # <-- CORREÇÃO (adicionado zero_division)\n",
        "recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "\n",
        "print(\"\\nMétricas do modelo SVM:\")\n",
        "print(f\"Acurácia: {accuracy_svm:.4f}\")\n",
        "print(f\"Precisão: {precision_svm:.4f}\")\n",
        "print(f\"Recall: {recall_svm:.4f}\")\n",
        "print(f\"F1-Score: {f1_svm:.4f}\")\n",
        "\n",
        "\n",
        "# 5.3.2. Multi-Layer Perceptron (MLP):\n",
        "print(\"\\n5.3.2. Treinando modelo MLP...\")\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1\n",
        ")\n",
        "mlp_model.fit(X_train, y_train) # y_train agora é numérico, o erro não ocorrerá\n",
        "print(\"Modelo MLP treinado com sucesso!\")\n",
        "\n",
        "# Predições\n",
        "y_pred_mlp = mlp_model.predict(X_test)\n",
        "\n",
        "# Métricas\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "precision_mlp = precision_score(y_test, y_pred_mlp, average='weighted', zero_division=0) # <-- CORREÇÃO (adicionado zero_division)\n",
        "recall_mlp = recall_score(y_test, y_pred_mlp, average='weighted')\n",
        "f1_mlp = f1_score(y_test, y_pred_mlp, average='weighted')\n",
        "\n",
        "print(\"\\nMétricas do modelo MLP:\")\n",
        "print(f\"Acurácia: {accuracy_mlp:.4f}\")\n",
        "print(f\"Precisão: {precision_mlp:.4f}\")\n",
        "print(f\"Recall: {recall_mlp:.4f}\")\n",
        "print(f\"F1-Score: {f1_mlp:.4f}\")\n",
        "\n",
        "# 5.4. Salvando modelos:\n",
        "print(\"\\n5.4. Salvando modelos treinados...\")\n",
        "with open('svm_model_corrected.pkl', 'wb') as f:\n",
        "    pickle.dump(svm_model, f)\n",
        "print(\"Modelo SVM salvo como 'svm_model_corrected.pkl'\")\n",
        "\n",
        "with open('mlp_model_corrected.pkl', 'wb') as f:\n",
        "    pickle.dump(mlp_model, f)\n",
        "print(\"Modelo MLP salvo como 'mlp_model_corrected.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inU0tZMFujuX",
        "outputId": "c9543d85-7103-4cc4-a00a-0e11aa7b19af"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target (y) para classificação: 'Rotulo_Custo'\n",
            "Features (X) para classificação: ['Country', 'City', 'University', 'Program', 'Level', 'Duration_Years']\n",
            "\n",
            "Rótulos do Target codificados (ex: 'Custo Alto' -> 0)\n",
            "\n",
            "Distribuição das classes (Target: Rotulo_Custo):\n",
            "Rotulo_Custo\n",
            "Custo Medio-Baixo    343\n",
            "Custo Baixo          256\n",
            "Custo Medio-Alto     210\n",
            "Custo Alto            98\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Detectado desbalanceamento. Aplicando downsampling...\n",
            "\n",
            "Distribuição das classes após balanceamento (rótulos codificados):\n",
            "Rotulo_Custo\n",
            "0    98\n",
            "1    98\n",
            "2    98\n",
            "3    98\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Tamanho do conjunto de treino: 313 exemplos\n",
            "Tamanho do conjunto de teste: 79 exemplos\n",
            "Tipo de y_train: int64\n",
            "\n",
            "5.3.1. Treinando modelo SVM...\n",
            "Modelo SVM treinado com sucesso!\n",
            "\n",
            "Métricas do modelo SVM:\n",
            "Acurácia: 0.4304\n",
            "Precisão: 0.7898\n",
            "Recall: 0.4304\n",
            "F1-Score: 0.3995\n",
            "\n",
            "5.3.2. Treinando modelo MLP...\n",
            "Modelo MLP treinado com sucesso!\n",
            "\n",
            "Métricas do modelo MLP:\n",
            "Acurácia: 0.2278\n",
            "Precisão: 0.2052\n",
            "Recall: 0.2278\n",
            "F1-Score: 0.1983\n",
            "\n",
            "5.4. Salvando modelos treinados...\n",
            "Modelo SVM salvo como 'svm_model_corrected.pkl'\n",
            "Modelo MLP salvo como 'mlp_model_corrected.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Aplicação em dados não rotulados (CORRIGIDO)\n",
        "\n",
        "# O objetivo é usar os modelos para prever a FAIXA DE CUSTO de novos dados.\n",
        "\n",
        "# 'X_test' contém nossos dados de teste (features não-financeiras, codificadas).\n",
        "# 'y_test' contém os rótulos reais (numéricos) correspondentes.\n",
        "X_unlabeled = X_test.iloc[:5].copy()\n",
        "y_real_encoded = y_test.iloc[:5].copy() # Estes são os rótulos numéricos (ex: 0, 1, 2)\n",
        "\n",
        "print(\"\\n6.1. Aplicando os classificadores treinados em 5 amostras de teste...\")\n",
        "\n",
        "# --- INÍCIO DA CORREÇÃO: Carregar o Encoder do Target ---\n",
        "# Precisamos dele para traduzir as previsões numéricas (0,1,2) de volta para texto\n",
        "try:\n",
        "    with open('target_label_encoder.pkl', 'rb') as f:\n",
        "        le_target = pickle.load(f)\n",
        "    print(\"Encoder de rótulos (target) carregado com sucesso.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRO: Arquivo 'target_label_encoder.pkl' não encontrado.\")\n",
        "    print(\"Por favor, rode a Célula 5 novamente para criar o arquivo.\")\n",
        "    # Fallback: criar um encoder falso para evitar que o resto do código quebre\n",
        "    le_target = LabelEncoder()\n",
        "    le_target.fit(['Fallback_Label_1', 'Fallback_Label_2']) # Apenas para o código rodar\n",
        "# --- FIM DA CORREÇÃO ---\n",
        "\n",
        "\n",
        "# 1. Prevendo classes (Faixa de Custo) utilizando os modelos treinados:\n",
        "try:\n",
        "    # Modelos preveem os rótulos numéricos\n",
        "    svm_predictions_encoded = svm_model.predict(X_unlabeled)\n",
        "    mlp_predictions_encoded = mlp_model.predict(X_unlabeled)\n",
        "    print(\"Predições (numéricas) realizadas com sucesso.\")\n",
        "\n",
        "    # --- CORREÇÃO: Traduzir previsões numéricas para texto ---\n",
        "    svm_predictions = le_target.inverse_transform(svm_predictions_encoded)\n",
        "    mlp_predictions = le_target.inverse_transform(mlp_predictions_encoded)\n",
        "    # Traduzir também os valores reais para a tabela de comparação\n",
        "    y_real = le_target.inverse_transform(y_real_encoded)\n",
        "    print(\"Previsões traduzidas de volta para texto (ex: 'Custo Alto').\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro inesperado durante a predição ou tradução: {e}\")\n",
        "    svm_predictions = [\"ERRO\"] * len(X_unlabeled)\n",
        "    mlp_predictions = [\"ERRO\"] * len(X_unlabeled)\n",
        "    y_real = [\"ERRO\"] * len(X_unlabeled)\n",
        "\n",
        "\n",
        "# 2. Exibindo os resultados de forma clara\n",
        "print(\"\\n--- Previsões de FAIXA DE CUSTO para novos dados ---\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Exemplo':<10} | {'Previsão SVM':<18} | {'Previsão MLP':<18} | {'Valor Real':<18}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Iterar usando os índices para garantir o alinhamento\n",
        "for i in range(len(X_unlabeled)):\n",
        "    svm_pred = svm_predictions[i]\n",
        "    mlp_pred = mlp_predictions[i]\n",
        "    real_val = y_real[i] # .iloc[i] não é necessário em arrays numpy\n",
        "\n",
        "    print(f\"{i+1:<10} | {svm_pred:<18} | {mlp_pred:<18} | {real_val:<18}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRXjGiwutdH",
        "outputId": "d7def848-b2fd-4606-ef5a-2093fb7824b0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6.1. Aplicando os classificadores treinados em 5 amostras de teste...\n",
            "Encoder de rótulos (target) carregado com sucesso.\n",
            "Predições (numéricas) realizadas com sucesso.\n",
            "Previsões traduzidas de volta para texto (ex: 'Custo Alto').\n",
            "\n",
            "--- Previsões de FAIXA DE CUSTO para novos dados ---\n",
            "----------------------------------------------------------------------\n",
            "Exemplo    | Previsão SVM       | Previsão MLP       | Valor Real        \n",
            "----------------------------------------------------------------------\n",
            "1          | Custo Alto         | Custo Baixo        | Custo Alto        \n",
            "2          | Custo Medio-Alto   | Custo Baixo        | Custo Medio-Alto  \n",
            "3          | Custo Baixo        | Custo Alto         | Custo Medio-Baixo \n",
            "4          | Custo Baixo        | Custo Baixo        | Custo Medio-Baixo \n",
            "5          | Custo Baixo        | Custo Baixo        | Custo Medio-Baixo \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Conclusão (CORRIGIDA)\n",
        "\n",
        "print(\"\\n\\n7. Conclusão\")\n",
        "\n",
        "# --- CORREÇÃO: Ajustar o loop para o novo formato de 'semantic_labels' ---\n",
        "try:\n",
        "    print(f\"Foram identificados {optimal_k} clusters de CUSTO, que foram mapeados para:\")\n",
        "\n",
        "    # Nosso 'semantic_labels' é um dict simples: {cluster_id: 'label_string'}\n",
        "    for cluster_id, label in semantic_labels.items():\n",
        "        print(f\" - Cluster {cluster_id}: {label}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao exibir informações de clusters: {str(e)}\")\n",
        "    print(\"Verifique se as variáveis 'optimal_k' e 'semantic_labels' da Célula 4 existem.\")\n",
        "# --- FIM DA CORREÇÃO ---\n",
        "\n",
        "\n",
        "print(\"\\nDesempenho dos modelos de classificação:\")\n",
        "print(\"(Tarefa: Prever a Faixa de Custo com base em País, Curso, Duração, etc.)\")\n",
        "print(f\"SVM: Acurácia = {accuracy_svm:.4f}, F1-Score = {f1_svm:.4f}\")\n",
        "print(f\"MLP: Acurácia = {accuracy_mlp:.4f}, F1-Score = {f1_mlp:.4f}\")\n",
        "# (Nota: As métricas agora são realistas e não 100%, o que é um SUCESSO)\n",
        "\n",
        "# Salvando:\n",
        "try:\n",
        "    if isinstance(dataset, pd.DataFrame):\n",
        "        # O 'dataset' agora contém as colunas 'Cluster' e 'Rotulo_Custo'\n",
        "        dataset.to_csv('credit_data_processed.csv', index=False)\n",
        "        print(f\"\\nDataset processado salvo como 'credit_data_processed.csv'\")\n",
        "    else:\n",
        "        print(\"AVISO: dataset não é um DataFrame, não foi possível salvar.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao salvar dataset: {str(e)}\")\n",
        "\n",
        "print(\"\\nPipeline corrigido finalizado!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGB1o0fYuy6l",
        "outputId": "ff2b07cd-6104-47cb-8552-54b63a0cc5c5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "7. Conclusão\n",
            "Foram identificados 4 clusters de CUSTO, que foram mapeados para:\n",
            " - Cluster 3: Custo Baixo\n",
            " - Cluster 0: Custo Medio-Baixo\n",
            " - Cluster 2: Custo Medio-Alto\n",
            " - Cluster 1: Custo Alto\n",
            "\n",
            "Desempenho dos modelos de classificação:\n",
            "(Tarefa: Prever a Faixa de Custo com base em País, Curso, Duração, etc.)\n",
            "SVM: Acurácia = 0.4304, F1-Score = 0.3995\n",
            "MLP: Acurácia = 0.2278, F1-Score = 0.1983\n",
            "\n",
            "Dataset processado salvo como 'credit_data_processed.csv'\n",
            "\n",
            "Pipeline corrigido finalizado!\n"
          ]
        }
      ]
    }
  ]
}